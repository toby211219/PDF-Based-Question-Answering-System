{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT2-i16p6uZv",
        "outputId": "c82fb4fc-7bca-40c6-b307-5dd9a4c17ac8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m174.1/232.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts all text from the specified PDF file.\n",
        "    Args:\n",
        "        pdf_path (str): The path to the PDF file.\n",
        "    Returns:\n",
        "        str: All text extracted from the PDF, or None if an error occurred.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page in reader.pages:\n",
        "                extracted_page_text = page.extract_text()\n",
        "                if extracted_page_text:\n",
        "                    # Attempt to add two newlines after each page's text to help with segmentation\n",
        "                    text += extracted_page_text + \"\\n\\n\"\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def split_text_into_chunks_fixed_length(text, chunk_size=500, chunk_overlap=100):\n",
        "    \"\"\"\n",
        "    Splits text into chunks of fixed length with overlap.\n",
        "    Args:\n",
        "        text (str): The complete text string.\n",
        "        chunk_size (int): The maximum number of characters per text chunk.\n",
        "        chunk_overlap (int): The number of overlapping characters between adjacent chunks.\n",
        "    Returns:\n",
        "        list: A list of strings, where each string is a text chunk.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    # Clean the text: remove extra whitespace, form feeds, etc.\n",
        "    # Replace all newlines with a single space, then handle extra spaces\n",
        "    cleaned_text = text.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text) # Replace multiple spaces with a single space\n",
        "\n",
        "    chunks = []\n",
        "    start_index = 0\n",
        "    while start_index < len(cleaned_text):\n",
        "        end_index = start_index + chunk_size\n",
        "        if end_index > len(cleaned_text):\n",
        "            chunks.append(cleaned_text[start_index:])\n",
        "            break\n",
        "\n",
        "        # Attempt to cut at the end of a sentence to avoid truncating sentences\n",
        "        # Find the nearest period, question mark, or exclamation point near chunk_size\n",
        "        split_point = cleaned_text.rfind('.', start_index, end_index)\n",
        "        if split_point == -1: # If no period is found in the current chunk\n",
        "            split_point = cleaned_text.rfind('。', start_index, end_index) # Try Chinese period\n",
        "        if split_point == -1: # If still not found, look for a space within the overlap range\n",
        "             split_point = cleaned_point = cleaned_text.rfind(' ', start_index, end_index)\n",
        "\n",
        "\n",
        "        if split_point > start_index + chunk_overlap: # Ensure the split point is within a reasonable range\n",
        "            current_chunk = cleaned_text[start_index:split_point + 1].strip()\n",
        "            chunks.append(current_chunk)\n",
        "            start_index = split_point + 1 - chunk_overlap\n",
        "        else:\n",
        "            # If there's no good split point, just split by chunk_size\n",
        "            current_chunk = cleaned_text[start_index:end_index].strip()\n",
        "            chunks.append(current_chunk)\n",
        "            start_index += chunk_size - chunk_overlap\n",
        "\n",
        "        # Ensure start_index does not become negative\n",
        "        start_index = max(0, start_index)\n",
        "\n",
        "    # Filter out any potentially empty chunks\n",
        "    return [chunk for chunk in chunks if chunk]\n",
        "\n",
        "# Example usage:\n",
        "pdf_file_path = \"AI Team2 7.4報告.pdf\" # Use the PDF file name you provided\n",
        "full_pdf_text = extract_text_from_pdf(pdf_file_path)\n",
        "\n",
        "if full_pdf_text:\n",
        "    print(\"Successfully extracted text from PDF.\")\n",
        "\n",
        "    # Here we use fixed-length splitting and specify chunk size and overlap\n",
        "    # You can adjust chunk_size and chunk_overlap based on your PDF content and needs\n",
        "    chunks = split_text_into_chunks_fixed_length(full_pdf_text, chunk_size=300, chunk_overlap=50)\n",
        "\n",
        "    print(f\"Total {len(chunks)} text chunks split.\")\n",
        "    print(\"\\n--- First 3 text chunks ---\")\n",
        "    for i, chunk in enumerate(chunks[:3]):\n",
        "        print(f\"Text chunk {i+1}:\\n{chunk}\\n---\")\n",
        "\n",
        "    print(\"\\n--- Last 3 text chunks ---\")\n",
        "    for i, chunk in enumerate(chunks[-3:]):\n",
        "        print(f\"Text chunk {len(chunks) - 3 + i + 1}:\\n{chunk}\\n---\")\n",
        "\n",
        "    # You can choose to store these chunks in a variable for subsequent tasks\n",
        "    # segmented_text_chunks = chunks\n",
        "else:\n",
        "    print(\"Failed to extract text from PDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWgRXVni6vdO",
        "outputId": "041f7e30-e1af-4873-89b8-0ca99126332d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-20') invalid; use 0.0 instead\n",
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-20') invalid; use 0.0 instead\n",
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-20') invalid; use 0.0 instead\n",
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-20') invalid; use 0.0 instead\n",
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-20') invalid; use 0.0 instead\n",
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-20') invalid; use 0.0 instead\n",
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-20') invalid; use 0.0 instead\n",
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-20') invalid; use 0.0 instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted text from PDF.\n",
            "Total 9 text chunks split.\n",
            "\n",
            "--- First 3 text chunks ---\n",
            "Text chunk 1:\n",
            "AI Identiﬁcation of Chinese Medicine Members : Hung Lung-Chen, Yu Pin-Yi, Hsieh Ching-Hung, Chen Kai-Jin Team2 Project : Introduction What is Chinese medicine? ●Uses herbs based on traditional Chinese medical theory. ●Comes from plants , animals , and minerals .\n",
            "---\n",
            "Text chunk 2:\n",
            "ory. ●Comes from plants , animals , and minerals . ●Primary purpose is disease prevention and maintaining health . Introduction Our solution ●Use AI to identify Chinese medicinal herbs accurately . Why are we doing this project? ●Many types of Chinese medicinal herbs look very similar.\n",
            "---\n",
            "Text chunk 3:\n",
            "ypes of Chinese medicinal herbs look very similar. ●Chinese medicine is becoming increasingly popular worldwide. ● Key Features Advantage ●Quickly and accurately obtain information about Chinese medicine. ●Provides an additional option to help protect your health and defend against diseases .\n",
            "---\n",
            "\n",
            "--- Last 3 text chunks ---\n",
            "Text chunk 7:\n",
            "ccuracy Results ：Our model achieved 80% accuracy . Identiﬁcation results Ginseng ⭕ Angelica Root ⭕ Astragalus Root ⭕ White Peony Root ⭕ Chuanxiong Rhizome ❌ Poria ⭕ Licorice Root ⭕ Pinellia Tuber ⭕ Bitter Orange ❌ Achyranthes Root ⭕ Identiﬁcation results Platycodon Root ⭕ Raw Rehmannia Root ❌ Prepar\n",
            "---\n",
            "Text chunk 8:\n",
            "ults Platycodon Root ⭕ Raw Rehmannia Root ❌ Prepared Rehmannia Root ⭕ Schisandra Fruit ⭕ Eucommia Bark ⭕ Sappan Wood ⭕ Dendrobium ❌ Gastrodia Rhizome ⭕ Cinnamon Bark ⭕ Coix Seed ⭕ ○ Conclusion Summary ●Model accuracy: 80% ●Easy to use for general public ●Professionals may require higher precision\n",
            "---\n",
            "Text chunk 9:\n",
            "ublic ●Professionals may require higher precision Future Work ●Build a curated database ●Improve classiﬁcation for better accuracy Thank you for your attention! Q&A\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}